{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935d0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7ca06",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff05ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDatasets():\n",
    "    # Change the path according to your system configuration\n",
    "    path = r\"C:/Users/afs18/Downloads/\"\n",
    "    real_folders = ['data/2017/real_accounts1.csv', 'data/2015/real_accounts2.csv', 'data/2015/real_accounts3.csv']\n",
    "    bot_folders = ['data/2017/bots1.csv',\n",
    "                         'data/2017/bots2.csv',\n",
    "                         'data/2017/bots3.csv',\n",
    "                         'data/2017/bots4.csv']\n",
    "    real_users_list = []\n",
    "    real_tweet_list = []  \n",
    "    for folder in real_folders:\n",
    "        df_users = pd.read_csv(path + folder+\"/users.csv\",index_col=\"id\", encoding=\"latin-1\", low_memory=False);\n",
    "        df_users[\"source_f\"] = folder; \n",
    "        real_users_list.append(df_users);\n",
    "        df_tweets = pd.read_csv(path + folder+\"/tweets.csv\",index_col=\"id\", encoding=\"latin-1\", low_memory=False);\n",
    "        df_tweets[\"source_f\"] = folder; \n",
    "        real_tweet_list.append(df_tweets);\n",
    "    real_tweets = pd.concat(real_tweet_list)\n",
    "    real_users = pd.concat(real_users_list)\n",
    "    real_users[\"is_bot\"] = 0\n",
    "    \n",
    "    bot_users_list = []\n",
    "    bot_tweets_list = []\n",
    "    for folder in bot_folders:\n",
    "        df_users = pd.read_csv(path + folder+\"/users.csv\",index_col=\"id\", encoding=\"latin-1\", low_memory=False);\n",
    "        df_users[\"source_f\"] = folder;\n",
    "        bot_users_list.append(df_users);    \n",
    "        df_tweets = pd.read_csv(path + folder+\"/tweets.csv\",index_col=\"id\", encoding=\"latin-1\", low_memory=False);\n",
    "        df_tweets[\"source_f\"] = folder;\n",
    "        bot_tweets_list.append(df_tweets);\n",
    "    bot_tweets = pd.concat(bot_tweets_list)\n",
    "    bot_users = pd.concat(bot_users_list)\n",
    "    bot_users[\"is_bot\"] = 1\n",
    "    \n",
    "    users = pd.concat([bot_users,real_users])\n",
    "    tweets = pd.concat([bot_tweets,real_tweets])\n",
    "\n",
    "    # Remove users with no tweets\n",
    "    users[\"is_bot\"] = users[\"is_bot\"][users.index.isin(tweets.set_index(\"user_id\").index)]\n",
    "    users = users[users.index.isin(tweets.set_index(\"user_id\").index)]\n",
    "    \n",
    "    del users[\"following\"];\n",
    "    del tweets[\"contributors\"];\n",
    "    del users[\"notifications\"];\n",
    "    del tweets[\"favorited\"];\n",
    "    del tweets[\"geo\"];\n",
    "    del users[\"contributors_enabled\"];\n",
    "    del users[\"follow_request_sent\"];    \n",
    "    del users[\"test_set_1\"];\n",
    "    del users[\"test_set_2\"];\n",
    "    del tweets[\"retweeted\"];\n",
    "    \n",
    "    # Change timestamp into datetime\n",
    "    tweets[\"timestamp_dt\"] = pd.to_datetime(tweets[\"timestamp\"],format='%Y-%m-%d %H:%M:%S')\n",
    "    del tweets[\"timestamp\"]\n",
    "\n",
    "    return users, tweets                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e8bd0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "users, tweets = createDatasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375e820",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdfe1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEngineering(users, tweets):\n",
    "    # Add a new feature which is the ratio of friends count to followers counts\n",
    "    users.loc[:,\"friend_follower_ratio\"] = users[\"friends_count\"] / (users[\"followers_count\"] + 1)\n",
    "    \n",
    "    \n",
    "    # Get the average of the tweets features corresponding to every user\n",
    "    average_tweets = tweets[[\"num_hashtags\", \"retweet_count\", \"favorite_count\", \"num_mentions\", \"num_urls\", \"user_id\"]].groupby(\"user_id\").mean()\n",
    "    \n",
    "    # Replace missing values with 0\n",
    "    average_tweets = average_tweets.fillna(0)\n",
    "    \n",
    "    # Add _per_tweet to the features names\n",
    "    users = users.join(average_tweets.add_suffix(\"_per_tweet\"))\n",
    "    \n",
    "    users.loc[:,\"unique_tweet_place\"] = tweets.groupby(\"user_id\").place.nunique()\n",
    "    \n",
    "    # Add a new feature which is the variance of user's tweets per hour\n",
    "    tweets.loc[:,\"date\"] = tweets.timestamp_dt.dt.date\n",
    "    tweets.loc[:,\"hour\"] = tweets.timestamp_dt.dt.hour\n",
    "    variance_tweet_rate = tweets.groupby([\"user_id\", \"date\", \"hour\"]).size().groupby(\"user_id\").var()\n",
    "    variance_tweet_rate.rename(\"variance_in_tweet_rate\", inplace=True)\n",
    "    users = users.join(variance_tweet_rate)\n",
    "    \n",
    "    return users, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9db4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "users, tweets = featureEngineering(users, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d315186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection(users):\n",
    "    final_df = users[['favourites_count','followers_count','friends_count','verified','friend_follower_ratio','num_hashtags_per_tweet','retweet_count_per_tweet','favorite_count_per_tweet','num_mentions_per_tweet','num_urls_per_tweet','unique_tweet_place','variance_in_tweet_rate', 'is_bot']]\n",
    "    values = {\"verified\":0, \"variance_in_tweet_rate\":0}\n",
    "    final_df = final_df.fillna(value=values)\n",
    "    X = final_df.iloc[:,:-1]\n",
    "    y = final_df.iloc[:, -1:]\n",
    "    return X, y\n",
    "\n",
    "X, y = featureSelection(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e136f",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bca2bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(X, isTrain=True):\n",
    "    if(isTrain):\n",
    "        scaler = StandardScaler()\n",
    "        scaler = scaler.fit(X)\n",
    "        pickle.dump(scaler, open(\"Pickle/scaler.pickle\", \"wb\"))\n",
    "        X.loc[:,:] = scaler.transform(X)\n",
    "    else:\n",
    "        scaler = pickle.load(open(\"Pickle/scaler.pickle\", \"rb\"))\n",
    "        X.loc[:,:] = scaler.transform(X)\n",
    "        \n",
    "    return X\n",
    "X = normalizeData(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29401ac",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79a0cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "pickle.dump(X_train, open(\"Pickle/X_train.pickle\", \"wb\"))\n",
    "pickle.dump(X_val, open(\"Pickle/X_val.pickle\", \"wb\"))\n",
    "pickle.dump(y_train, open(\"Pickle/y_train.pickle\", \"wb\"))\n",
    "pickle.dump(y_val, open(\"Pickle/y_val.pickle\", \"wb\"))\n",
    "pickle.dump(X_test, open(\"Pickle/X_test.pickle\", \"wb\"))\n",
    "pickle.dump(y_test, open(\"Pickle/y_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f451dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 5364\n",
      "y_train: 5364\n",
      "X_val: 1788\n",
      "y_val: 1788\n",
      "X_test: 1789\n",
      "y_test: 1789\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: {}\".format(len(X_train)))\n",
    "print(\"y_train: {}\".format(len(y_train)))\n",
    "print(\"X_val: {}\".format(len(X_val)))\n",
    "print(\"y_val: {}\".format(len(y_val)))\n",
    "print(\"X_test: {}\".format(len(X_test)))\n",
    "print(\"y_test: {}\".format(len(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
